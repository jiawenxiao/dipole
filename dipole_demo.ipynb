{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:53:16.747852Z",
     "start_time": "2020-01-17T07:53:10.612624Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "from keras.layers import merge\n",
    "from keras.layers.recurrent import LSTM\n",
    "import keras.backend as K\n",
    "from keras.layers import BatchNormalization,concatenate,Flatten,Concatenate,Activation,Conv1D,Add ,MaxPooling1D,Masking,GRU\n",
    "from keras.layers.core import Lambda, Dense, Activation\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam,RMSprop    \n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dropout,GlobalMaxPooling1D\n",
    "from keras.callbacks import Callback,EarlyStopping\n",
    "from keras.layers.core import *\n",
    "from tensorflow.contrib import rnn \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:56:42.807125Z",
     "start_time": "2020-01-17T07:56:37.831224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1719: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6631903326511384\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "n_hidden =12\n",
    "embDimSize=6\n",
    "timeseq=32\n",
    "is_attention=True\n",
    "\n",
    "x =np.random.random((3200,32,6)).astype(np.float32)\n",
    "y =np.vstack([np.zeros((1600,1)),np.ones((1600,1))]).astype(np.float32)\n",
    "\n",
    "tf.reset_default_graph() \n",
    "data   = tf.placeholder(tf.float32, [None,timeseq, embDimSize],name='data')\n",
    "label  = tf.placeholder(tf.float32, [None,1],name='target') \n",
    "\n",
    "\n",
    "gru_fw_cell=rnn.GRUCell(n_hidden,activation='tanh')\n",
    "gru_bw_cell=rnn.GRUCell(n_hidden,activation='tanh')\n",
    "outputs,output_states=tf.nn.bidirectional_dynamic_rnn(gru_fw_cell,gru_bw_cell,data,dtype=tf.float32)\n",
    "\n",
    "outputs_concat = tf.concat(outputs,2)\n",
    "atten_weight = Dense(1, activation='softmax')(outputs_concat) #在时间序列上添加注意力\n",
    "output_attention= outputs_concat*atten_weight\n",
    "output_attention_mean =tf.reduce_sum(output_attention,axis=2) \n",
    "\n",
    "current_hidden=output_states[0]\n",
    "\n",
    "h_t = tf.concat([output_attention_mean ,current_hidden],axis=1)\n",
    "h_t = tf.nn.tanh(h_t)\n",
    "y_hat = Dense(1)(h_t)\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label,logits=y_hat))\n",
    "solver = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss) \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "total_batch=int(x.shape[0]/batch_size)\n",
    "total_cost=0\n",
    "\n",
    "attention_total=np.zeros((0,32,1))\n",
    "for i in range(total_batch):\n",
    "    x_batch=x[i*batch_size:(i+1)*batch_size]\n",
    "    y_batch=y[i*batch_size:(i+1)*batch_size]\n",
    "    _,current_loss,current_atten_weight,temp=sess.run([solver, loss,atten_weight,outputs_concat],  feed_dict={data:x_batch,label:y_batch}) \n",
    "    attention_total=np.vstack([attention_total,current_atten_weight])\n",
    "        \n",
    "    total_cost+=current_loss\n",
    "    avg_cost=total_cost/(i+1)\n",
    "\n",
    "    \n",
    "#查看平均注意力\n",
    "avg_attention_weight=np.mean(attention_total,axis=0)\n",
    "print(avg_cost)\n",
    "# print(avg_attention_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
